---
layout: post
title: "Reflection on "Common misconceptions about data analysis and statistics""
date: 2021-02-04
---
Motulsky (2014) claims that inadequate comprehension of statistical concepts has led to low reproducibility of published findings. 
Among major factors, there are two common mistakes that investigators make when performing statistical analysis. As a Statistics student, 
I should keep these in mind to avoid the same mistakes in future work. 

The first one is P-Hacking. It generally stands for situations when analysts keep introducing new data or adapting other methodologies until the 
results are statistically significant and expected as before the analysis. One straightforward example would be hypothesizing after the result is 
known (Kerr, 1998). The results generated after this kind of process are inappropriate to be interpreted at face value because of the biases added 
when reanalyzing (Motulsky, 2014). To avoid misleading output, Motulsky (2014) suggests clearly stating the experimental planning and performing process. 
Otherwise, the conclusions would be preliminary. From my perspective, it is easy to form a biased conjecture when analyzing data. I used to repeat modifying 
models in school projects to discuss variables that seem correct. I wasn't aware of the risks of P-Hacking, which could sharply reduce the reliability of my work.

Statistical hypothesis testing is the other source of unreproducible reports. Granted, the method is contributive under certain situations like in clinical. 
Investigators usually emphases on the 5% significance cutoff and strictly apply it to their experimental research. According to the example created by 
Motulsky (2014), the probability of making type I errors could reach up to 79% instead of 5%. Consequently, it is apprehensible that most published findings 
are not reproducible. With this in mind, Motulsky (2014) found many scientists replacing the word "significant" with other less assertive ones such as 
"almost significant," "a statistical trend toward significance," and "approaching significance." He also advises all investigators to include the exact number 
for P-value, the threshold used, and the decision made (Motulsky, 2014). I used to take the 5% significance threshold as granted and as a strict cutoff, leading 
to the repetition of changing model variables to attain my expected p-values. P-values should only be a reference instead of a standard, and they should never 
impact experiment designs.

In conclusion, mistakes like P-hacking and overreliance on statistical hypothesis testing will lower the paper reproducibility. Scientists should avoid these two 
factors by stating the details of the experiment and explaining all the decisions. Personally, I should remember these potential mistakes when performing analysis 
and producing results in my future work.


Reference:

Kerr NL (1998) HARKing: hypothesizing after the results are known. Pers Soc Psychol Rev 2:196–217 

Motulsky, H. J. (2014). Common misconceptions about data analysis and statistics. Naunyn-Schmiedeberg’s Archives of Pharmacology, 387(11), 1017–1023. https://doi.org/10.1007/s00210-014-1037-6




