---
layout: post
title: "Explain Maximum Likelihood Estimation (MLE)"
date: 2021-02-04
---

Sure, I can explain it shortly. Basically, maximum likelihood estimation is a method used to find values for parameters of a probability function. 
And these values are that maximized the probability of the observed data occurring under the assumed model.

Let me give you an example of maximum likelihood. For instance, I have one dice here, and there is no other information. You may think it is a standard dice, 
and the probability of getting a number from 1 to 6 is the same, which would be one-sixth. However, if I told you that I got 100 sixes for 100 tosses or 1000 
sixes for 1000 tosses. You may guess that the dice is unique, and the probability of getting a six would be 100%. Your guess changed here from around 16% to 
100% because it is the most possible probability here, given the tossing data. The likelihood is a function that has parameters as variables. You do understand 
that we will have different probability functions for the maximum likelihood estimation, right? So for various functions, we would have different maximum 
likelihood estimator that gives the most likely parameter value for our data.

Remember the properties for the estimators? For a maximum likelihood estimator, it could be biased. However, as the number of samples increases, biases will close 
to zero. They have the smallest variance among all unbiased estimators. There are others as well.

I will explain in detail next time!
